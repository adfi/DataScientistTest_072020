---
title: "Data Science Test July 2020"
author: "Adnan Fiaz"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

This document outlines the answer to the Data Science Test as described in _README.md_. In short, the task is to build a model to predict the death of a set of patients. We will proceed by first performing basic data analysis to clean the data and then perform a number of model iterations including feature engineering if necessary. 

```{r libraries}
library(tidyverse)
library(tidymodels)
```


## Cleaning data

We start by reading in the data and calculating summary statistics. We already know _Death_ is a categorical variable so we force it to a factor, along with all categorical variables.


```{r read data}
raw_data <- read_csv('simulated_data.csv') %>%
  mutate(Death = factor(Death, labels=c("survived", "died"))) %>%
  mutate_if(is.character, as.factor)

raw_data
```

```{r skim data}
library(skimr)
skim(raw_data)
```

As mentioned in the instructions there are indeed 300 observations and 6 columns. Starting with the outcome column, _Death_, we see there is an imbalance in the data with the majority of the observations belonging to the _survived_ category. We also see there is an equal number of observations from each of the 10 values in _Organisation_. Finally, the majority of the observations belong to the _Low_ value from the _Category_ variable but the number of observations in the other values is not insignificant.

As for the numeric variables, we see that _Age_ has a very wide range from 5 to 95 but a fairly even distribution. However, the length-of-stay or _LOS_ variable is skewed to the left with a majority of the observations having a short length-of-stay. Without any further information we assume the values in _LOS_ are acceptable. 

Overall, there are no missing or out of the ordinary values so we proceed without any cleaning of the data.

### Data Analysis

We continue with the analysis with some basic plots to uncover any insights that might help us in the modeling steps. First, a look at the categorical variables in relation to the outcome variable. 

```{r count by organisation, echo=FALSE}
raw_data %>%
  ggplot(aes(x=Organisation, fill=Death)) +
  geom_bar(stat='count') + 
  labs(title="Number of survived/died patients by Organisation", y='Number of patients')
```

Since there's an equal number of observations in each organisation splitting by the outcome variables makes for an easy comparison. The proportion of survived patients clearly varies by organisation. However we also see some organisations having roughly the same proportion such as _Trust4_, _Trust6_, _Trust9_ and _Trust10_. To reduce the levels of this categorical variable it might be useful to cluster some organisations although I think there isn't enough information in the dataset to do that. Clustering would also be useful if more organisations were added in the future, rather than retraining the model you could then just assign any new organisation to an appropriate cluster.


```{r count by category, echo=FALSE}
raw_data %>%
  ggplot(aes(x=Category, fill=Death)) +
  geom_bar(stat='count', position='fill') + 
  labs(title="Proportion of survived/died patients by Category", y='Proportion of patients')
```

For the _Category_ variable we look at the proportion of patients as their quantity differs by value. Here we see that a _Low_ risk indeed means a lower proportion of deaths. The proportion isn't very different for the _High_ and _Moderate_ values. It will be interesting to see how that impacts the modeling.

Since we mentioned clustering the organisations it is perhaps interesting to see how _Category_ differs by organisation. 

```{r proportion by organisation and category, echo=FALSE}
raw_data %>%
  ggplot(aes(x=Organisation, fill=Category)) +
  geom_bar(stat='count', position='fill') +
  labs(title="Proportion patients by organisation and category", y="Proportion of patients")
```

There are organisations which have similar proportions of the _Category_ variable such as _Trust3_ and _Trust4_ but their proportion of the outcome variable still differs. Also _Trust7_ springs out with significantly different proportions yet its proportion of the outcome variable is only slightly different from other organisations. 

We continue looking at the numeric variables.

```{r boxplot age, echo=FALSE}
raw_data %>%
  ggplot(aes(x=Death, y=Age, fill=Death)) +
  geom_violin() +
  guides(fill=FALSE) +
  labs(title="Age distribution by outcome")
```

For the _Age_ variable we see that for the _died_ outcome the distribution is skewed towards higher values. However the distribution for the _survived_ outcome seems to be bimodal. It looks like our earlier observation of a fairly even _Age_ distribution wasn't entirely true, the overall distribution looks closer to a bimodal distribution with a peak at young and old ages. 

```{r boxplot los, echo=FALSE}
raw_data %>%
  ggplot(aes(x=Death, y=LOS, fill=Death)) +
  geom_violin() + 
  guides(fill=FALSE) + 
  labs(title="Length of stay distribution by outcome")
```

For the _LOS_ variable we also see different distributions for the two outcome. For the _survived_ value the distribution is similar to the overall distribution, namely left skewed. For the _died_ value the distribution is more stretched out and exhibits more values for longer length of stays.


Altogether, this basic data analysis hasn't uncovered anything unusual requiring us to clean or manipulate the data. The variables all seem to have some predictive value so we don't need to drop any of them either. We could do further analysis of the data looking at more cross-sections (e.g. Age by Category) but we have sufficient information to proceed to the modelling step so we'll leave that for now. 

## Modelling data

Granted we could've skipped the above data analysis and gone straight to building a model but it's equally important to get to know the data (and make pretty plots in the process). Now that we are here, we first need to establish how we will model this challenge. Since the outcome variable is binary we will need to build a classification model. As for the metric to assess model performance, we will use ROC and AUC as these are "assumption-free" metrics. Metrics such as F1-score or accuracy require assumptions about the relative importance of the outcomes and/or probability cut-off.

To get an unbiased value for the metric we split the data into a training and test set.
```{r split data}
set.seed(20200803)

raw_data_split <- initial_split(raw_data, prop = 0.8, strata=Death)

training_data <- training(raw_data_split)
test_data <- testing(raw_data_split)
```


### Model Iteration 1: The baseline

Now that we know where we're headed let's see how we can get there really quick. It's important to establish a simple baseline model before we start adding complexity. The simplest classification model that I can think of is a logistic regression. 

```{r baseline}
# define preprocessing (none in this case)
baseline_recipe <- 
  recipe(Death ~ ., data=training_data) %>%
  update_role(ID, new_role="ID") # ensures the ID isn't used in the model without removing it

# define model
baseline_model <- 
  logistic_reg() %>%
  set_engine("glm")

# bundle model and preprocessing
baseline_wf <- workflow() %>%
  add_recipe(baseline_recipe) %>%
  add_model(baseline_model)

baseline_wf
```

The above seems a bit convoluted but using the **tidymodels** framework does make it easier to iterate over models as we will see later. Next we will fit the model using 3-fold cross validation. We could just fit it once but with cross validation we get a better estimate of the unbiased metric.

```{r baseline cv}
set.seed(20200803) # set random generator seed to ensure reproducibility

baseline_wf %>%
  fit_resamples(vfold_cv(training_data, v=3, strata=Death),
                metrics = metric_set(roc_auc)) %>%
  collect_metrics() %>%
  mutate(model="baseline") %>%
  select(model, auc=mean) -> leaderboard

leaderboard
```

So our baseline model performs better than a model where the outcome is randomly chosen (AUC=0.5). That's great but let's see if we can improve on this.

### Model Iteration 2: Preprocessing

Preprocessing is a type of feature engineering, we're providing information to the model on how to better learn from the input variables. In this case we first need to convert the categorical variables to dummy variables. Our numerical variables, _Age_ and _LOS_ are also on different scales so we can center and scale these. We will do all of this by adjusting the _recipe_.

```{r workflow with preprocessing}
preproces_recipe <- baseline_recipe %>%
  step_normalize(all_numeric()) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors()) # remove any variables that have zero variance

preprocess_wf <-
  baseline_wf %>%
  update_recipe(preproces_recipe)
```

```{r cv with preprocessing}
set.seed(20200803) # set seed again to ensure we get the same CV folds
# at this point you should make a function of this
preprocess_wf %>%
  fit_resamples(vfold_cv(training_data, v=3, strata=Death),
                metrics = metric_set(roc_auc)) %>%
  collect_metrics() %>%
  mutate(model="preprocess_lr") %>%
  select(model, auc=mean) %>%
  bind_rows(leaderboard) -> leaderboard

leaderboard
```

It seems that preprocessing doesn't have any added benefit. This could be because of two reasons: there is no actual difference in the model (i.e. there is preprocessing happening in the baseline model) or there is no actual difference in performance. The example [documentation](https://www.tidymodels.org/start/recipes/#features) states the baseline recipe we created doesn't do any preprocessing so we'll go with the second reason. We could investigate the model fits (using `pull_worklfow_fit`) to be sure but I'll leave that for another time as this isn't the final model iteration.

### Model Iteration 3: The Random Forest

Now is the time to add some more complexity. We could go two ways here, add more features or try another algorithm. Trying another algorithm is quicker to do so we'll start with that. Since we'll be using a tree-based algorithm we'll skip the preprocessing as tree-based algorithms are flexible enough to handle the data without it.

```{r rf spec}
rf_model <- 
  rand_forest() %>%
  set_engine(engine='ranger') %>%
  set_mode('classification')

rf_wf <- 
  baseline_wf %>%
  update_model(rf_model)
```

```{r cv with rf}
set.seed(20200803)
# I really wish I had made a function of this
rf_wf %>%
  fit_resamples(vfold_cv(training_data, v=3, strata=Death),
                metrics = metric_set(roc_auc)) %>%
  collect_metrics() %>%
  mutate(model="rf") %>%
  select(model, auc=mean) %>%
  bind_rows(leaderboard) -> leaderboard

leaderboard
```

The Random Forest outperforms the logistic regression based on the AUC. Now we can train it on the entire dataset and apply it to the test data to get a final performance value.

```{r final model}
set.seed(20200803)
rf_wf %>%
  last_fit(split=raw_data_split, metrics=metric_set(roc_auc)) %>%
  collect_metrics()
```


## Follow up

We performed three model iterations in which the Random Forest performed best. There are various reasons why it performed better than the logistic regression model but clearly there is some complexity or information in the data that isn't being captured by the logistic regression model. Possible next steps are:

* Analyse performance of the best model by looking at which observations are missclassified. What information is not being captured? Which variables does the model find are most important? Since Random Forests are black box models we could supplement this analysis with LIME and/or Shapley values to find out how predictions were made.
* Add more features. Although the Random Forest should be able to capture any non-linear information we could make things more explicit. For example, we know Age has a bimodal distribution and we can use this to create new features.
* Since the dataset was imbalanced with respect to the outcome variable we could try balancing the dataset by upsampling or downsampling.
* Tune hyperparameters for the best model. In the case of the Random Forest that could be the number of trees (`trees`) or the number of variables to be considered at each split (`mtry`).

## Addendum: Model Cards

I recently came across the concept of [Model Cards](https://modelcards.withgoogle.com/about) and I'm trying to apply it in practice wherever possible. What particularly attracts me is the importance of describing the limits of the model and the model data. Below is an initial attempt at creating a model card for the Random Forest.


```{r model card, echo=FALSE}
library(gt)

tribble(
  ~Name, ~Value,
  'Goal:', 'Predict whether a patient will survive',
  'Input:', 'Organisation, Category, Age and Length of Stay (LOS)',
  'Output', 'A value from {"survived", "died"}',
  'Model Architecture:', 'Random Forest trained with the {ranger} package, default settings',
  'Performance:', '0.71 Test AUC (59 observations)',
  'Limitations:', '5 <= Age <= 95',
  '', '1 <= LOS <= 18',
  '', '10 Organisations',
  '', '3 Categories'
) %>%
  gt() %>%
  tab_options(table.align = "left", column_labels.hidden=TRUE,
              table.border.top.style='solid',
              table.border.right.style='solid',
              table.border.left.style='solid',
              table_body.border.bottom.style='none',
              table.border.top.color='black',
              table.border.right.color='black',
              table.border.left.color='black',
              table.border.bottom.color='black',
              heading.border.bottom.color = 'black', 
              table_body.hlines.style = "none",) %>%
  tab_style(style = cell_text(weight = 'bold'), 
            location = cells_body(columns='Name')) %>%
  tab_header(title="Model Card Predict Patient Death")
```
